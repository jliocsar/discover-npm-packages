{ argv, exit } from node:process
{ fetch } from undici
pino from pino
pretty from pino-pretty
minimist from minimist
removeMd from remove-markdown
{ Octokit } from octokit
type { TPackageNode, TPackage } from ./types.civet
{ curry, ascByDownloads } from ./utils.civet

NPM_REGISTRY_URL := 'https://registry.npmjs.org'
NPM_DOWNLOADS_API_URL := 'https://api.npmjs.org/downloads/point/last-month'
NPM_PACKAGE_URL := 'https://www.npmjs.com/package'
HARD_CRAWL_LIMIT := 5

octokit := new Octokit auth: process.env.GITHUB_PERSONAL_ACCESS_TOKEN!
logger := pino pretty colorize: true, singleLine: true

{ _: packages, crawl: crawlCount } := minimist argv.slice(2), alias: { c: 'crawl' }, default: { crawl: 2 }
adjustedCrawlCount .= crawlCount

if adjustedCrawlCount > HARD_CRAWL_LIMIT
  logger.warn `Crawl limit is too high! Max. is ${HARD_CRAWL_LIMIT}`
  exit 1

logger.info `Running for ${packages.length} packages, using crawl limit of ${adjustedCrawlCount}`

getNpmPackage := (pkg: string) ->
  fetch `${NPM_REGISTRY_URL}/${pkg}/latest` |> await |> .json()
getNpmPackagesDownloads := (pkg: string) ->
  fetch `${NPM_DOWNLOADS_API_URL}/${pkg}` |> await |> .json()
getReadme := (owner: string, repo: string) ->
  octokit.rest.repos.getReadme { owner, repo }
    |> await
    |> .data.content
    |> (content: string) -> Buffer.from content, 'base64'
    |> .toString()
getReadmePreview := (repositoryUrl: string) ->
  [owner, repo] := repositoryUrl
    |> .replace /^git:\/\/github\.com\//, ''
    |> .replace /\.git$/, ''
    |> .split '/'
  getReadme owner, repo
    |> await
    |> removeMd
    |> .slice 0, 1000
    |> & + '...'

isGithub := (npmPkg: TPackage) -> 'repository' in npmPkg
  and /^git:\/\/github\.com/.test npmPkg.repository.url

crawl := curry (crawls = 0, pkgs: string[]) ->
  await.all
    for pkg of pkgs
      npmPkg := (await getNpmPackage pkg) as TPackage
      { downloads } := (await getNpmPackagesDownloads pkg) as { downloads: number }
      node : TPackageNode := {
        downloads
        name: pkg
        description: npmPkg.description
        url: NPM_PACKAGE_URL + '/' + pkg
        readmePreview: if isGithub npmPkg then
          await getReadmePreview npmPkg.repository.url
      }
      unless crawls is adjustedCrawlCount
        node.children = Object.assign {}, npmPkg.dependencies, npmPkg.devDependencies, npmPkg.peerDependencies
          |> Object.keys
          |> crawl crawls + 1
          |> await
          |> .sort ascByDownloads
      logger.info node
      node as TPackageNode

main := ->
  packages
    |> crawl()
    |> await
    |> (data) -> Bun.write
      'output.json'
      JSON.stringify data, null, 2

await main()
