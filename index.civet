{ argv } from node:process
{ fetch } from undici
pino from pino
pretty from pino-pretty
minimist from minimist
type { TPackageNode } from ./types.civet

NPM_REGISTRY_URL := 'https://registry.npmjs.org'
NPM_DOWNLOADS_API_URL := 'https://api.npmjs.org/downloads/point/last-month'
NPM_PACKAGE_URL := 'https://www.npmjs.com/package'
HARD_CRAWL_LIMIT := 5

logger := pino pretty colorize: true, singleLine: true

{ _: packages, crawl } := minimist argv.slice(2), alias: { c: 'crawl' }
crawlLimit .= crawl ?? 2

if crawlLimit > HARD_CRAWL_LIMIT
  logger.warn `Crawl limit is too high, setting to ${HARD_CRAWL_LIMIT}`
  crawlLimit = HARD_CRAWL_LIMIT

logger.info `Running for ${packages.length} packages, using crawl limit of ${crawlLimit}`

findNpmPackage := (pkg: string) ->
  fetch `${NPM_REGISTRY_URL}/${pkg}/latest` |> await |> .json()

findNpmPackagesDownloads := (pkg: string): Promise<any> ->
  fetch `${NPM_DOWNLOADS_API_URL}/${pkg}` |> await |> .json()

byDownloads := (orig: TPackageNode, comp: TPackageNode) ->
  comp.downloads - orig.downloads

transverse := (crawls = 0) -> (pkgs: string[]): Promise<TPackageNode[]> ->
  await.all
    for pkg of pkgs
      { downloads } := await findNpmPackagesDownloads pkg
      node: Partial<TPackageNode> .= {
        downloads,
        name: pkg,
        url: NPM_PACKAGE_URL + '/' + pkg,
      }
      if crawls is not crawlLimit
        children := findNpmPackage pkg
          |> await
          |> (item: any) => Object.assign {}, item.dependencies, item.devDependencies, item.peerDependencies
          |> Object.keys
          |> transverse crawls + 1
          |> await
        Object.assign node, children: children.sort byDownloads
      logger.info node
      node as TPackageNode
  .sort byDownloads

main := ->
  packages
    |> transverse()
    |> await
    |> (data) => Bun.write 'output.json', JSON.stringify(data, null, 2)

await main()
